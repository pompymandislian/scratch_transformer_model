{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyPB74qu1UWfXFFdWNkI4WxA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pompymandislian/scratch_transformer_model/blob/main/Transformer_Scratch%20(V2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-_lO8aMMt6-n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Step by Step Transformer**\n",
        "---"
      ],
      "metadata": {
        "id": "ypZvHRZAVGQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "      def __init__(self, embed_size, heads): # embed 512 , heads 8 relational\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads # if divide not zero\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False) # similarity token\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False) # high attention\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False) # output\n",
        "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size) # calculate final output\n",
        "\n",
        "      def forward(self, values, keys, query, mask):\n",
        "          \"\"\"\n",
        "          Process input data through multihead attention to produce weighted output.\n",
        "\n",
        "          Parameters:\n",
        "          -----------\n",
        "          values : torch.Tensor\n",
        "              Tensor containing the values to be attended to.\n",
        "\n",
        "          keys : torch.Tensor\n",
        "              Tensor containing the keys used to calculate attention scores with the queries.\n",
        "\n",
        "          query : torch.Tensor\n",
        "              Tensor containing the queries used to compute attention scores with the keys.\n",
        "\n",
        "          mask : torch.Tensor, optional\n",
        "              A tensor of shape (batch_size, query_len, key_len), where each element is either 0 or 1.\n",
        "\n",
        "          Returns:\n",
        "          --------\n",
        "          torch.Tensor\n",
        "              The output of the multihead attention layer, where the attention scores have been applied to the values.\n",
        "          \"\"\"\n",
        "          N = query.shape[0] # batch size\n",
        "          value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "          # note: shape[0] is batch size, shape[1] is sequence lenght token\n",
        "          # shape[2] is heads, shape[3] is head_dim\n",
        "\n",
        "          # split embedding into self.heads --> split each heads (4 dimentions)\n",
        "          values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "          keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "          queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "          # calculate heads with pararel\n",
        "          energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "          # obtain attantion, mask words not know just know words\n",
        "          if mask is not None: # find in dict data if nothing from result Q V K then 0\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "          # obtain probability attention\n",
        "          attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "          # Apply attention scores to the values to get the weighted output\n",
        "          # multiple dimension\n",
        "          out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "              N, query_len, self.heads*self.head_dim\n",
        "          )\n",
        "\n",
        "          # final output\n",
        "          out = self.fc_out(out)\n",
        "          return out"
      ],
      "metadata": {
        "id": "0f6Z8EiwuHX7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing class SelfAttention\n",
        "embed_size = 512\n",
        "heads = 8\n",
        "self_attention = SelfAttention(embed_size, heads)\n",
        "\n",
        "print(self_attention.values)\n",
        "print(self_attention.keys)\n",
        "print(self_attention.queries)\n",
        "print(self_attention.fc_out)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pXAp49SvHrt",
        "outputId": "203c34ca-5ef4-46ac-fc24-0c46bc8d8832"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear(in_features=64, out_features=64, bias=False)\n",
            "Linear(in_features=64, out_features=64, bias=False)\n",
            "Linear(in_features=64, out_features=64, bias=False)\n",
            "Linear(in_features=512, out_features=512, bias=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing forward\n",
        "vocab = {\n",
        "    \"I\": 0,\n",
        "    \"love\": 1,\n",
        "    \"programming\": 2,\n",
        "    \"is\": 3,\n",
        "    \"fun\": 4,\n",
        "    \"machine\": 5,\n",
        "    \"learning\": 6,\n",
        "    \"very\": 7,\n",
        "    \"exciting\": 8,\n",
        "    \"and\": 9\n",
        "}\n",
        "\n",
        "id_to_token = dict(map(reversed, vocab.items())) # dict\n",
        "\n",
        "# assume tokens id\n",
        "token_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 0],\n",
        "                          [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
        "\n",
        "\n",
        "# vocab token id (dict), 10 because we have 10 token\n",
        "embedding = nn.Embedding(len(vocab), embed_size)\n",
        "\n",
        "print(embedding)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8mLOEDFl3e8W",
        "outputId": "c252d498-39da-41da-a47a-de5a9bb752e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(10, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define V K Q\n",
        "values = embedding(token_ids)\n",
        "keys = embedding(token_ids)\n",
        "query = embedding(token_ids)\n",
        "\n",
        "mask = None\n",
        "\n",
        "# obtain output attention\n",
        "output = self_attention(values, keys, query, mask)\n",
        "\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkZ0OXsf6yBb",
        "outputId": "2a9b940d-b667-466d-ba4b-9dc71a362199"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "      def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "          super(TransformerBlock, self).__init__()\n",
        "          self.attention = SelfAttention(embed_size, heads)\n",
        "\n",
        "          # normalize for stabilization data\n",
        "          self.norm1 = nn.LayerNorm(embed_size)\n",
        "          self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "          # feed forward arschitecture\n",
        "          self.feed_forward = nn.Sequential(\n",
        "              nn.Linear(embed_size, forward_expansion * embed_size), # flexible feed\n",
        "              nn.ReLU(), # change to positive (non linear)\n",
        "              nn.Linear(forward_expansion * embed_size, embed_size) # flexible feed\n",
        "          )\n",
        "\n",
        "      def forward(self, values, keys, query, mask):\n",
        "          \"\"\"Feed Forward V K Q\"\"\"\n",
        "          # Compute attention output based on query, keys, and values\n",
        "          attention = self.attention(values, keys, query, mask)\n",
        "\n",
        "          # Skip connection: add attention output with input query, then normalize\n",
        "          x = self.norm1(attention + query)\n",
        "\n",
        "          # Feed forward with the normalized output of attention + query\n",
        "          forward = self.feed_forward(x)\n",
        "\n",
        "          # Skip connection again: add FFN output with previous input, then normalize\n",
        "          out = self.norm2(forward + x)\n",
        "\n",
        "          return out"
      ],
      "metadata": {
        "id": "nXxhZnGrPc-d"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing class TransformerBlock\n",
        "forward_expansion = 4 # dimension layer\n",
        "dropout=0.1\n",
        "\n",
        "transformer = TransformerBlock(embed_size, heads,\n",
        "                               dropout, forward_expansion)\n",
        "\n",
        "print(transformer.attention)\n",
        "print(transformer.norm1)\n",
        "print(transformer.norm2)\n",
        "print(transformer.feed_forward)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0mR1ucnSAAG",
        "outputId": "c63d8c16-e0fa-47f6-e994-96a53e89855b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttention(\n",
            "  (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "  (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "  (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "  (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            ")\n",
            "LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "Sequential(\n",
            "  (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "  (1): ReLU()\n",
            "  (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# testing output forward\n",
        "output = transformer(values, keys, query, mask)\n",
        "\n",
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vzM2lSBGUTw5",
        "outputId": "fa14870a-a9b4-4e38-d2ac-2374135ae721"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "      def __init__(self, src_vocab_size, embed_size,\n",
        "                  num_layers, heads, device,\n",
        "                  forward_expansion, dropout, max_length):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "\n",
        "        # not using sinusoida because we need training data\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "              TransformerBlock(embed_size, heads,\n",
        "                               dropout=dropout,\n",
        "                               forward_expansion=forward_expansion)\n",
        "              # looping layer\n",
        "              for _ in range(num_layers)\n",
        "          ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, mask = None):\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X : torch.tensor\n",
        "          input data\n",
        "\n",
        "        mask : torch.tensor\n",
        "          mask data\n",
        "\n",
        "        Return:\n",
        "        -------\n",
        "        torch.tensor\n",
        "          output data\n",
        "        \"\"\"\n",
        "        # sum of length words\n",
        "        N, seq_length = x.shape\n",
        "\n",
        "        # position values\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "        # embedding to vektor embedding and summation with position\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # result of out continue to layers\n",
        "        for layer in self.layers:\n",
        "          out = layer(out, out, out, mask)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "UDN90antVLKy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test class Encoder\n",
        "src_vocab_size = 10000  # size vocab input\n",
        "embed_size = 512  # Dimensi embedding\n",
        "num_layers = 1  # layer transformer block\n",
        "heads = 8  # Multihead Attention\n",
        "device = 'cpu'  # Device\n",
        "forward_expansion = 4  # dimension layer\n",
        "dropout = 0.1  # Dropout rate\n",
        "max_length = 60  # max sort lenght\n",
        "\n",
        "encoder = Encoder(src_vocab_size, embed_size, num_layers,\n",
        "                  heads, device, forward_expansion, dropout, max_length)\n",
        "\n",
        "print(encoder.embed_size)\n",
        "print(encoder.device)\n",
        "print(encoder.word_embedding)\n",
        "print(encoder.position_embedding)\n",
        "print(encoder.layers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWGEqNFfW5l5",
        "outputId": "fde742df-29fd-4be2-e305-251f7a180c7d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "512\n",
            "cpu\n",
            "Embedding(10000, 512)\n",
            "Embedding(60, 512)\n",
            "ModuleList(\n",
            "  (0): TransformerBlock(\n",
            "    (attention): SelfAttention(\n",
            "      (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "      (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "      (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "      (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "    )\n",
            "    (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (feed_forward): Sequential(\n",
            "      (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "      (1): ReLU()\n",
            "      (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test forward\n",
        "encoder.forward(token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_y2tcezhaqCf",
        "outputId": "ff1de323-6cf1-40d3-8227-d595a971bd9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.6894, -1.0818, -0.0805,  ..., -0.2028, -1.3544, -0.6048],\n",
              "         [ 0.3709, -0.9345,  1.4319,  ..., -0.6475,  0.8177,  2.3684],\n",
              "         [-1.2066,  0.4468, -1.4281,  ..., -1.7451,  1.2988, -0.1292],\n",
              "         ...,\n",
              "         [ 0.7149, -0.6914, -0.5641,  ...,  0.4712, -0.4231, -0.9916],\n",
              "         [-0.2099, -0.5330,  0.0212,  ...,  0.3055, -1.7120, -0.5566],\n",
              "         [ 0.1003,  0.1165,  1.1143,  ...,  0.5316,  0.8280,  0.8455]],\n",
              "\n",
              "        [[ 0.6437, -0.9746,  0.3059,  ..., -0.4748, -0.6032, -0.8995],\n",
              "         [-0.0262, -2.5942, -0.4762,  ...,  0.3753, -0.2043,  0.8237],\n",
              "         [-0.0856, -0.2041,  0.4204,  ..., -1.5958,  1.4535,  0.4304],\n",
              "         ...,\n",
              "         [ 1.2091, -0.6240, -0.7827,  ...,  2.5901, -0.4674, -1.5980],\n",
              "         [ 1.7856, -1.6918, -0.2146,  ..., -0.9761, -2.3058, -0.3542],\n",
              "         [-0.9477,  0.4265,  1.1593,  ...,  0.7040,  0.3397, -0.6253]]],\n",
              "       grad_fn=<NativeLayerNormBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "      def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "          super(DecoderBlock, self).__init__()\n",
        "\n",
        "          self.attention = SelfAttention(embed_size, heads=heads)\n",
        "          self.norm = nn.LayerNorm(embed_size)\n",
        "          self.transformer_block = TransformerBlock(\n",
        "              embed_size, heads, dropout, forward_expansion)\n",
        "\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, value, key, trg_mask, src_mask=None):\n",
        "          \"\"\"Forward pass for block decoder (not relevant values)\"\"\"\n",
        "\n",
        "          # find data is not process in each K Q V with trg_mask\n",
        "          attention = self.attention(x, x, x, trg_mask)\n",
        "\n",
        "          # streamlining gradien prevent lose information\n",
        "          query = self.dropout(self.norm(attention + x))\n",
        "\n",
        "          # calculate attention and block inrelevant word with src_mask\n",
        "          out = self.transformer_block(value, key, query, src_mask)\n",
        "\n",
        "          return out"
      ],
      "metadata": {
        "id": "-EuD2EdIbF3a"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test class DecoderBlock\n",
        "batch_size = 2\n",
        "seq_len = 10\n",
        "embed_size = 512\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "# embedding token_ids\n",
        "embedding_layer = nn.Embedding(len(vocab), embed_size).to(device)\n",
        "embedded_tokens = embedding_layer(token_ids).to(device)  # Shape: (batch_size, seq_len, embed_size)\n",
        "\n",
        "# Mask\n",
        "trg_mask = torch.tril(torch.ones((batch_size, seq_len, seq_len))).to(device)  # Causal mask (shape: batch_size, seq_len, seq_len)\n",
        "trg_mask = trg_mask.unsqueeze(1)  # change dimension (batch_size, 1, seq_len, seq_len)\n",
        "\n",
        "# Output encoder embedding is same\n",
        "encoder_output = embedded_tokens\n",
        "\n",
        "# Membuat DecoderBlock\n",
        "decoder_block = DecoderBlock(embed_size, heads=8,\n",
        "                             forward_expansion=4,\n",
        "                             dropout=0.1,\n",
        "                             device=device).to(device)\n",
        "\n",
        "print(decoder_block.attention)\n",
        "print(decoder_block.norm)\n",
        "print(decoder_block.transformer_block)\n",
        "print(decoder_block.dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVh4lp6ldFla",
        "outputId": "7e5ec347-9dc1-4809-809f-08f2e405892a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SelfAttention(\n",
            "  (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "  (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "  (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "  (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            ")\n",
            "LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "TransformerBlock(\n",
            "  (attention): SelfAttention(\n",
            "    (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "    (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "    (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "    (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "  (feed_forward): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "  )\n",
            ")\n",
            "Dropout(p=0.1, inplace=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward decoder\n",
        "decoder_output = decoder_block.forward(embedded_tokens,\n",
        "                                       encoder_output,\n",
        "                                       encoder_output, trg_mask)\n",
        "\n",
        "print(\"Decoder Output Shape:\", decoder_output.shape)  # Output: (batch_size, seq_len, embed_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwpiINDajl5Y",
        "outputId": "20b7abe7-6728-4b09-ce60-6f864b6a4bd7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoder Output Shape: torch.Size([2, 10, 512])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "      def __init__(self, trg_vocab_size, embed_size,\n",
        "                   num_layers, heads,\n",
        "                   forward_expansion,\n",
        "                   dropout, device, max_length):\n",
        "\n",
        "          super(Decoder, self).__init__()\n",
        "          self.device = device\n",
        "\n",
        "          # target data to vector\n",
        "          self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "\n",
        "          # position for data target\n",
        "          self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "          # architecture\n",
        "          self.layers = nn.ModuleList([\n",
        "              DecoderBlock(embed_size, heads,\n",
        "                           forward_expansion, dropout, device)\n",
        "              for _ in range(num_layers)\n",
        "          ])\n",
        "\n",
        "          # fully connection target data\n",
        "          self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, enc_out, trg_mask, src_mask=None):\n",
        "          n, seq_length = x.shape\n",
        "\n",
        "          # position values\n",
        "          positions = torch.arange(0, seq_length).expand(n, seq_length).to(self.device)\n",
        "\n",
        "          # embedding to vektor embedding and summation with position\n",
        "          x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "          # loop layers\n",
        "          for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, trg_mask, src_mask)\n",
        "\n",
        "          # final output\n",
        "          out = self.fc_out(x)\n",
        "\n",
        "          return out"
      ],
      "metadata": {
        "id": "RxuAMNbrmLTQ"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test class decoder\n",
        "trg_vocab_size = len(vocab) # length of vocab\n",
        "\n",
        "# decoder\n",
        "decoder = Decoder(trg_vocab_size, embed_size, num_layers, heads,\n",
        "                  forward_expansion, dropout, device, max_length).to(device)\n",
        "\n",
        "\n",
        "print(decoder.word_embedding)\n",
        "print(decoder.position_embedding)\n",
        "print(decoder.layers)\n",
        "print(decoder.fc_out)\n",
        "print(decoder.dropout)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LfgEEnVNSsAM",
        "outputId": "50f6caf4-d101-4445-a14c-f4564be467cc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Embedding(10, 512)\n",
            "Embedding(60, 512)\n",
            "ModuleList(\n",
            "  (0): DecoderBlock(\n",
            "    (attention): SelfAttention(\n",
            "      (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "      (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "      (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "      (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "    )\n",
            "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "    (transformer_block): TransformerBlock(\n",
            "      (attention): SelfAttention(\n",
            "        (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "        (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "        (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "        (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "      )\n",
            "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (feed_forward): Sequential(\n",
            "        (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "        (1): ReLU()\n",
            "        (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            ")\n",
            "Linear(in_features=512, out_features=10, bias=True)\n",
            "Dropout(p=0.1, inplace=False)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test decoder forward\n",
        "decoder_output = decoder.forward(token_ids, encoder_output, trg_mask)\n",
        "\n",
        "print(decoder_output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrqQH_4lZLwo",
        "outputId": "255151ea-972d-4a8f-e9d3-ccb9eb31e760"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2, 10, 10])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "      def __init__(self, src_vocab_size, trg_vocab_size, device,\n",
        "                   src_pad_idx=0, trg_pad_idx=0,\n",
        "                   embed_size=512, num_layers=6, forward_expansion=4,\n",
        "                   heads=8, dropout=0, max_length=100):\n",
        "\n",
        "          super(Transformer, self).__init__()\n",
        "\n",
        "          # encoder process\n",
        "          self.encoder = Encoder(src_vocab_size,\n",
        "                                embed_size, num_layers, heads,\n",
        "                                device, forward_expansion,\n",
        "                                dropout, max_length)\n",
        "\n",
        "          # decoder process\n",
        "          self.decoder = Decoder(trg_vocab_size,\n",
        "                                embed_size, num_layers, heads,\n",
        "                                forward_expansion, dropout,\n",
        "                                device, max_length)\n",
        "\n",
        "          # find information not relevant\n",
        "          self.src_pad_idx = src_pad_idx\n",
        "          self.trg_pad_idx = trg_pad_idx\n",
        "          self.device = device\n",
        "\n",
        "      def make_src_mask(self, src):\n",
        "          \"\"\"Find mask from pad_idx\"\"\"\n",
        "          # find mask and create a pad_idx\n",
        "          src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "          # (N, 1, 1, src_len)\n",
        "          return src_mask.to(self.device)\n",
        "\n",
        "      def make_trg_mask(self, trg):\n",
        "          \"\"\"Hold words future\"\"\"\n",
        "          # obtain length\n",
        "          N, trg_len = trg.shape\n",
        "\n",
        "          # hold not used future words\n",
        "          trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "              N, 1, trg_len, trg_len\n",
        "          )\n",
        "\n",
        "          return trg_mask.to(self.device)\n",
        "\n",
        "      def forward(self, src, trg):\n",
        "          \"\"\"Running Transformer Model\"\"\"\n",
        "          # avoid attention padding\n",
        "          src_mask = self.make_src_mask(src)  # Mask untuk padding token pada source\n",
        "\n",
        "          # hold not see next future word\n",
        "          trg_mask = self.make_trg_mask(trg)  # Mask untuk target sequence\n",
        "\n",
        "          # process input with encoder\n",
        "          enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "          # process target with decoder\n",
        "          out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "          # Return output from decoder (not just trg_mask)\n",
        "          return out\n"
      ],
      "metadata": {
        "id": "uSbvbri0YlRJ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test class Transformer\n",
        "transformer = Transformer(src_vocab_size, trg_vocab_size, device)\n",
        "\n",
        "print('Encoder', transformer.encoder(token_ids))\n",
        "\n",
        "print('Decoder', transformer.decoder(token_ids, encoder_output, trg_mask))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwqFqYL7-b3p",
        "outputId": "cdfce660-b759-4f66-d4ad-7eb9b6bf28a3"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoder tensor([[[-1.5970, -0.6391,  2.2730,  ...,  0.2967, -0.6160,  0.8779],\n",
            "         [-1.1495, -0.9593,  0.4366,  ...,  1.3603, -0.4193,  1.0420],\n",
            "         [-0.4125, -1.2371,  0.0055,  ...,  0.2198, -1.5073,  2.5586],\n",
            "         ...,\n",
            "         [ 0.4063, -0.6674,  0.0942,  ...,  1.5814, -0.2556,  0.5165],\n",
            "         [-1.1757, -1.1254,  0.9289,  ...,  2.4127, -1.4108,  0.8692],\n",
            "         [-0.5346, -1.2139,  1.0224,  ...,  1.2773, -0.1418,  1.0747]],\n",
            "\n",
            "        [[-1.1141, -0.9489,  2.3258,  ...,  0.4645, -0.5379,  0.1532],\n",
            "         [-1.2585, -0.9842,  1.0694,  ...,  1.0049, -0.4197,  0.8696],\n",
            "         [-1.6852, -1.0057, -0.0076,  ...,  0.1520, -1.1211,  1.1090],\n",
            "         ...,\n",
            "         [-1.2036, -0.5035,  0.3905,  ...,  0.4053, -0.8536,  1.1632],\n",
            "         [ 0.4701, -0.8489,  0.1810,  ...,  1.9465, -0.0842,  1.1371],\n",
            "         [-0.4712, -2.1952,  1.1738,  ...,  3.3788, -1.3913,  1.2109]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n",
            "Decoder tensor([[[-0.0108, -1.0531,  0.1426, -0.1361,  0.3946, -0.6910,  0.8674,\n",
            "           0.8909,  0.4376, -1.1284],\n",
            "         [ 0.5418, -0.2953, -0.0436,  0.6517,  1.0454, -0.9196,  0.6640,\n",
            "           0.0202,  0.0396, -0.6107],\n",
            "         [ 0.0577, -0.2149, -0.7240,  0.6185,  0.8586, -0.1422,  0.8576,\n",
            "          -0.3142, -0.1280, -0.4616],\n",
            "         [-0.0810, -0.4664, -0.2845, -0.1474,  0.3217, -0.4472,  0.9254,\n",
            "          -0.0097, -0.1146, -0.6812],\n",
            "         [-0.2019, -0.1879, -0.9833, -0.6416,  0.8593, -0.2708,  1.5837,\n",
            "           0.3675, -0.2702, -1.6278],\n",
            "         [ 0.2516, -0.9123, -0.3936, -0.9012,  0.7330, -0.3676,  0.8508,\n",
            "          -1.0028,  0.2501, -0.9880],\n",
            "         [-0.2943,  0.0049,  0.1600, -0.2708,  0.8255,  0.0562,  1.5525,\n",
            "           0.3199, -0.4898, -1.7303],\n",
            "         [ 0.0208, -0.0394, -0.7170, -0.4009,  0.9604, -0.3541,  0.1926,\n",
            "           0.8355,  0.4735, -1.4356],\n",
            "         [-0.2893, -0.3833, -1.2251,  0.0370,  0.9208, -0.3177,  1.2882,\n",
            "           0.1046, -0.3582, -1.1442],\n",
            "         [-0.2367, -0.8234, -0.3380, -0.4442, -0.3627, -0.2215,  1.1398,\n",
            "           0.4959, -0.0774, -1.5168]],\n",
            "\n",
            "        [[-0.1389, -0.4418, -0.4334, -0.2821,  0.1188, -0.1642,  1.2737,\n",
            "           0.7236, -0.2270, -0.8153],\n",
            "         [-0.0754, -1.0692,  0.3749,  0.3117,  0.6244, -0.6172,  0.9444,\n",
            "           0.8117,  0.5056, -0.6079],\n",
            "         [ 0.4654, -0.3334, -0.8569,  0.4987,  0.4799, -0.3887,  1.3741,\n",
            "          -0.4952, -0.2136, -0.8170],\n",
            "         [ 0.2287, -0.1531,  0.1241,  0.0132,  0.4151, -0.0256,  1.5481,\n",
            "          -0.2027, -0.2105, -0.4173],\n",
            "         [-0.5289, -0.2323, -1.3817, -0.3449,  0.7111, -0.1642,  1.1395,\n",
            "           0.5373, -0.5022, -1.2860],\n",
            "         [-0.0660, -0.4954, -0.1843, -0.5382,  0.7592, -0.3263,  1.5528,\n",
            "          -0.5454, -0.1192, -1.1980],\n",
            "         [ 0.1291, -0.3322, -0.0253, -0.3614,  1.0703, -0.2272,  1.1054,\n",
            "          -0.5063, -0.3873, -1.1637],\n",
            "         [ 0.5180, -0.2445, -0.3100, -0.5203,  0.5523, -0.4092,  0.8082,\n",
            "           0.1735, -0.0896, -1.7009],\n",
            "         [-0.6101, -0.4264, -0.3853, -0.1967,  1.3207,  0.1106,  1.1160,\n",
            "           0.3483, -0.2696, -1.0179],\n",
            "         [-0.0726, -0.6518, -1.0974, -0.0438,  0.2356, -0.5517,  1.3450,\n",
            "           0.6540,  0.1784, -1.3467]]], grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test make src mask\n",
        "print(transformer.make_src_mask(token_ids))\n",
        "\n",
        "print(transformer.make_trg_mask(token_ids))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0CWzUa7B7pP",
        "outputId": "fa8c8994-734d-40f6-a0c8-677300152aa9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[[ True,  True,  True,  True,  True,  True,  True,  True,  True, False]]],\n",
            "\n",
            "\n",
            "        [[[False,  True,  True,  True,  True,  True,  True,  True,  True,  True]]]])\n",
            "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "        [[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
            "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# test forward\n",
        "transformer.forward(token_ids, token_ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpN3SOoYCgyb",
        "outputId": "88eac797-6408-434b-d4f7-e516595ceb77"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-0.2692, -0.9284,  0.3091, -0.6465, -0.4244, -0.2612,  0.3290,\n",
              "           0.3745,  0.7543,  0.0245],\n",
              "         [ 0.1243, -0.3863,  0.3331,  0.0285,  0.0913, -0.6686, -0.0174,\n",
              "          -0.1718,  0.3767,  0.2887],\n",
              "         [-0.1797, -0.3740, -0.2588,  0.1776,  0.0915,  0.0017,  0.1693,\n",
              "          -0.2255, -0.0292,  0.4454],\n",
              "         [-0.3091, -0.5568,  0.2743, -0.6087, -0.4043, -0.0398,  0.4076,\n",
              "           0.0028,  0.1441,  0.2490],\n",
              "         [-0.4135, -0.1826, -0.4783, -0.8689, -0.1955, -0.3868,  0.5608,\n",
              "           0.1947, -0.1306, -0.5409],\n",
              "         [-0.0736, -0.9116,  0.0348, -0.9732,  0.0981, -0.4088,  0.1292,\n",
              "          -0.9973,  0.5366,  0.0957],\n",
              "         [-0.5118, -0.2149,  0.6704, -0.5564, -0.0820, -0.1448,  0.5788,\n",
              "           0.0636, -0.1608, -0.6932],\n",
              "         [-0.3307, -0.1946, -0.3765, -0.3371,  0.0212, -0.2751, -0.2424,\n",
              "           0.4064,  0.7111, -0.2253],\n",
              "         [-0.3913, -0.5042, -0.6666, -0.3636, -0.0971, -0.2547,  0.4503,\n",
              "          -0.1842, -0.1087, -0.2123],\n",
              "         [-0.4827, -0.7728,  0.2140, -0.7225, -1.0150, -0.2831,  0.3111,\n",
              "           0.3526,  0.2007, -0.3754]],\n",
              "\n",
              "        [[-0.2524, -0.5992,  0.1631, -0.6516, -0.7960,  0.0763,  0.6079,\n",
              "           0.2994,  0.0132,  0.3834],\n",
              "         [-0.2615, -1.2834,  0.8685, -0.2378, -0.3642, -0.2829,  0.4032,\n",
              "           0.2149,  0.5238,  0.4485],\n",
              "         [ 0.1652, -0.5930, -0.0194, -0.1909, -0.5248, -0.1406,  0.7847,\n",
              "          -0.5425,  0.1163,  0.4000],\n",
              "         [ 0.1112, -0.5598,  0.7710, -0.4362, -0.3756,  0.2169,  0.8344,\n",
              "          -0.2049, -0.0672,  0.4930],\n",
              "         [-0.4623, -0.5390, -0.5067, -0.6684, -0.3442,  0.2008,  0.4866,\n",
              "           0.2652, -0.2952, -0.0950],\n",
              "         [-0.1792, -0.6933,  0.3680, -1.0184, -0.2403, -0.3360,  0.7291,\n",
              "          -0.5802,  0.0837,  0.1141],\n",
              "         [-0.1116, -0.6630,  0.7833, -0.6914,  0.0705, -0.1844,  0.4863,\n",
              "          -0.5644, -0.0873, -0.1393],\n",
              "         [ 0.1531, -0.3674,  0.3451, -0.7224, -0.4674, -0.2858, -0.0116,\n",
              "          -0.1898,  0.2162, -0.3983],\n",
              "         [-0.6443, -0.6776,  0.2285, -0.4583,  0.1068,  0.2112,  0.4176,\n",
              "          -0.1053, -0.0582,  0.1747],\n",
              "         [-0.1220, -0.9638, -0.1429, -0.4903, -0.5811, -0.4295,  0.6582,\n",
              "           0.1791,  0.2506, -0.0724]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full Code Transformer\n",
        "---"
      ],
      "metadata": {
        "id": "J-UAfgc9DuRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "      def __init__(self, embed_size, heads): # embed 512 , heads 8 relational\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads # if divide not zero\n",
        "\n",
        "        assert (self.head_dim * heads == embed_size), \"Embed size needs to be div by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False) # similarity token\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False) # high attention\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False) # output\n",
        "        self.fc_out = nn.Linear(heads*self.head_dim, embed_size) # calculate final output\n",
        "\n",
        "      def forward(self, values, keys, query, mask):\n",
        "          \"\"\"\n",
        "          Process input data through multihead attention to produce weighted output.\n",
        "\n",
        "          Parameters:\n",
        "          -----------\n",
        "          values : torch.Tensor\n",
        "              Tensor containing the values to be attended to.\n",
        "\n",
        "          keys : torch.Tensor\n",
        "              Tensor containing the keys used to calculate attention scores with the queries.\n",
        "\n",
        "          query : torch.Tensor\n",
        "              Tensor containing the queries used to compute attention scores with the keys.\n",
        "\n",
        "          mask : torch.Tensor, optional\n",
        "              A tensor of shape (batch_size, query_len, key_len), where each element is either 0 or 1.\n",
        "\n",
        "          Returns:\n",
        "          --------\n",
        "          torch.Tensor\n",
        "              The output of the multihead attention layer, where the attention scores have been applied to the values.\n",
        "          \"\"\"\n",
        "          N = query.shape[0] # batch size\n",
        "          value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "          # note: shape[0] is batch size, shape[1] is sequence lenght token\n",
        "          # shape[2] is heads, shape[3] is head_dim\n",
        "\n",
        "          # split embedding into self.heads --> split each heads (4 dimentions)\n",
        "          values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "          keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "          queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "          # calculate heads with pararel\n",
        "          energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
        "\n",
        "          # obtain attantion, mask words not know just know words\n",
        "          if mask is not None: # find in dict data if nothing from result Q V K then 0\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "          # obtain probability attention\n",
        "          attention = torch.softmax(energy / (self.embed_size ** (1/2)), dim=3)\n",
        "\n",
        "          # Apply attention scores to the values to get the weighted output\n",
        "          # multiple dimension\n",
        "          out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "              N, query_len, self.heads*self.head_dim\n",
        "          )\n",
        "\n",
        "          # final output\n",
        "          out = self.fc_out(out)\n",
        "          return out\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "      def __init__(self, embed_size, heads, dropout, forward_expansion):\n",
        "          super(TransformerBlock, self).__init__()\n",
        "          self.attention = SelfAttention(embed_size, heads)\n",
        "\n",
        "          # normalize for stabilization data\n",
        "          self.norm1 = nn.LayerNorm(embed_size)\n",
        "          self.norm2 = nn.LayerNorm(embed_size)\n",
        "\n",
        "          # feed forward arschitecture\n",
        "          self.feed_forward = nn.Sequential(\n",
        "              nn.Linear(embed_size, forward_expansion * embed_size), # flexible feed\n",
        "              nn.ReLU(), # change to positive (non linear)\n",
        "              nn.Linear(forward_expansion * embed_size, embed_size) # flexible feed\n",
        "          )\n",
        "\n",
        "      def forward(self, values, keys, query, mask):\n",
        "          \"\"\"Feed Forward V K Q\"\"\"\n",
        "          # Compute attention output based on query, keys, and values\n",
        "          attention = self.attention(values, keys, query, mask)\n",
        "\n",
        "          # Skip connection: add attention output with input query, then normalize\n",
        "          x = self.norm1(attention + query)\n",
        "\n",
        "          # Feed forward with the normalized output of attention + query\n",
        "          forward = self.feed_forward(x)\n",
        "\n",
        "          # Skip connection again: add FFN output with previous input, then normalize\n",
        "          out = self.norm2(forward + x)\n",
        "\n",
        "          return out\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "      def __init__(self, src_vocab_size, embed_size,\n",
        "                  num_layers, heads, device,\n",
        "                  forward_expansion, dropout, max_length):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.embed_size = embed_size\n",
        "        self.device = device\n",
        "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
        "\n",
        "        # not using sinusoida because we need training data\n",
        "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "        self.layers = nn.ModuleList([\n",
        "              TransformerBlock(embed_size, heads,\n",
        "                               dropout=dropout,\n",
        "                               forward_expansion=forward_expansion)\n",
        "              # looping layer\n",
        "              for _ in range(num_layers)\n",
        "          ])\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, mask = None):\n",
        "        \"\"\"\n",
        "        Forward pass of the encoder.\n",
        "\n",
        "        Parameters:\n",
        "        ----------\n",
        "        X : torch.tensor\n",
        "          input data\n",
        "\n",
        "        mask : torch.tensor\n",
        "          mask data\n",
        "\n",
        "        Return:\n",
        "        -------\n",
        "        torch.tensor\n",
        "          output data\n",
        "        \"\"\"\n",
        "        # sum of length words\n",
        "        N, seq_length = x.shape\n",
        "\n",
        "        # position values\n",
        "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
        "\n",
        "        # embedding to vektor embedding and summation with position\n",
        "        out = self.dropout(\n",
        "            (self.word_embedding(x) + self.position_embedding(positions))\n",
        "        )\n",
        "\n",
        "        # result of out continue to layers\n",
        "        for layer in self.layers:\n",
        "          out = layer(out, out, out, mask)\n",
        "\n",
        "        return out\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "      def __init__(self, embed_size, heads, forward_expansion, dropout, device):\n",
        "          super(DecoderBlock, self).__init__()\n",
        "\n",
        "          self.attention = SelfAttention(embed_size, heads=heads)\n",
        "          self.norm = nn.LayerNorm(embed_size)\n",
        "          self.transformer_block = TransformerBlock(\n",
        "              embed_size, heads, dropout, forward_expansion)\n",
        "\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, value, key, trg_mask, src_mask=None):\n",
        "          \"\"\"Forward pass for block decoder (not relevant values)\"\"\"\n",
        "\n",
        "          # find data is not process in each K Q V with trg_mask\n",
        "          attention = self.attention(x, x, x, trg_mask)\n",
        "\n",
        "          # streamlining gradien prevent lose information\n",
        "          query = self.dropout(self.norm(attention + x))\n",
        "\n",
        "          # calculate attention and block inrelevant word with src_mask\n",
        "          out = self.transformer_block(value, key, query, src_mask)\n",
        "\n",
        "          return out\n",
        "\n",
        "class Decoder(nn.Module):\n",
        "      def __init__(self, trg_vocab_size, embed_size,\n",
        "                   num_layers, heads,\n",
        "                   forward_expansion,\n",
        "                   dropout, device, max_length):\n",
        "\n",
        "          super(Decoder, self).__init__()\n",
        "          self.device = device\n",
        "\n",
        "          # target data to vector\n",
        "          self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
        "\n",
        "          # position for data target\n",
        "          self.position_embedding = nn.Embedding(max_length, embed_size)\n",
        "\n",
        "          # architecture\n",
        "          self.layers = nn.ModuleList([\n",
        "              DecoderBlock(embed_size, heads,\n",
        "                           forward_expansion, dropout, device)\n",
        "              for _ in range(num_layers)\n",
        "          ])\n",
        "\n",
        "          # fully connection target data\n",
        "          self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
        "          self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "      def forward(self, x, enc_out, trg_mask, src_mask=None):\n",
        "          n, seq_length = x.shape\n",
        "\n",
        "          # position values\n",
        "          positions = torch.arange(0, seq_length).expand(n, seq_length).to(self.device)\n",
        "\n",
        "          # embedding to vektor embedding and summation with position\n",
        "          x = self.dropout((self.word_embedding(x) + self.position_embedding(positions)))\n",
        "\n",
        "          # loop layers\n",
        "          for layer in self.layers:\n",
        "            x = layer(x, enc_out, enc_out, trg_mask, src_mask)\n",
        "\n",
        "          # final output\n",
        "          out = self.fc_out(x)\n",
        "\n",
        "          return out\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "      def __init__(self, src_vocab_size, trg_vocab_size, device,\n",
        "                   src_pad_idx=0, trg_pad_idx=0,\n",
        "                   embed_size=512, num_layers=6, forward_expansion=4,\n",
        "                   heads=8, dropout=0, max_length=100):\n",
        "\n",
        "          super(Transformer, self).__init__()\n",
        "\n",
        "          # encoder process\n",
        "          self.encoder = Encoder(src_vocab_size,\n",
        "                                embed_size, num_layers, heads,\n",
        "                                device, forward_expansion,\n",
        "                                dropout, max_length)\n",
        "\n",
        "          # decoder process\n",
        "          self.decoder = Decoder(trg_vocab_size,\n",
        "                                embed_size, num_layers, heads,\n",
        "                                forward_expansion, dropout,\n",
        "                                device, max_length)\n",
        "\n",
        "          # find information not relevant\n",
        "          self.src_pad_idx = src_pad_idx\n",
        "          self.trg_pad_idx = trg_pad_idx\n",
        "          self.device = device\n",
        "\n",
        "      def make_src_mask(self, src):\n",
        "          \"\"\"Find mask from pad_idx\"\"\"\n",
        "          # find mask and create a pad_idx\n",
        "          src_mask = (src != self.src_pad_idx).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "          # (N, 1, 1, src_len)\n",
        "          return src_mask.to(self.device)\n",
        "\n",
        "      def make_trg_mask(self, trg):\n",
        "          \"\"\"Hold words future\"\"\"\n",
        "          # obtain length\n",
        "          N, trg_len = trg.shape\n",
        "\n",
        "          # hold not used future words\n",
        "          trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
        "              N, 1, trg_len, trg_len\n",
        "          )\n",
        "\n",
        "          return trg_mask.to(self.device)\n",
        "\n",
        "      def forward(self, src, trg):\n",
        "          \"\"\"Running Transformer Model\"\"\"\n",
        "          # avoid attention padding\n",
        "          src_mask = self.make_src_mask(src)  # Mask untuk padding token pada source\n",
        "\n",
        "          # hold not see next future word\n",
        "          trg_mask = self.make_trg_mask(trg)  # Mask untuk target sequence\n",
        "\n",
        "          # process input with encoder\n",
        "          enc_src = self.encoder(src, src_mask)\n",
        "\n",
        "          # process target with decoder\n",
        "          out = self.decoder(trg, enc_src, trg_mask, src_mask)\n",
        "\n",
        "          # Return output from decoder (not just trg_mask)\n",
        "          return out"
      ],
      "metadata": {
        "id": "lvIMGrgjDEcw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execution Class\n",
        "---"
      ],
      "metadata": {
        "id": "LHyZ9q2hIUtC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "  print(device)\n",
        "\n",
        "  x = torch.tensor([[1, 5, 6,], [1, 3, 4]]).to(device)\n",
        "  trg = torch.tensor([[1, 5, 6,], [1, 3, 4]]).to(device)\n",
        "\n",
        "  src_pad_idx = 0\n",
        "  trg_pad_idx = 0\n",
        "  src_vocab_size = 10\n",
        "  trg_vocab_size = 10\n",
        "\n",
        "  model = Transformer(src_vocab_size, trg_vocab_size, device=device).to(device)\n",
        "\n",
        "  out = model(x, trg[:, :-1])\n",
        "  print(out.shape)\n",
        "\n",
        "  print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hD9Gov0bKVLp",
        "outputId": "bbcc4f4c-6e30-4554-bc54-18aea7a2ad24"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n",
            "torch.Size([2, 2, 10])\n",
            "Transformer(\n",
            "  (encoder): Encoder(\n",
            "    (word_embedding): Embedding(10, 512)\n",
            "    (position_embedding): Embedding(100, 512)\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x TransformerBlock(\n",
            "        (attention): SelfAttention(\n",
            "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (feed_forward): Sequential(\n",
            "          (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (1): ReLU()\n",
            "          (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (word_embedding): Embedding(10, 512)\n",
            "    (position_embedding): Embedding(100, 512)\n",
            "    (layers): ModuleList(\n",
            "      (0-5): 6 x DecoderBlock(\n",
            "        (attention): SelfAttention(\n",
            "          (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        (transformer_block): TransformerBlock(\n",
            "          (attention): SelfAttention(\n",
            "            (values): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (keys): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (queries): Linear(in_features=64, out_features=64, bias=False)\n",
            "            (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (feed_forward): Sequential(\n",
            "            (0): Linear(in_features=512, out_features=2048, bias=True)\n",
            "            (1): ReLU()\n",
            "            (2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (dropout): Dropout(p=0, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (fc_out): Linear(in_features=512, out_features=10, bias=True)\n",
            "    (dropout): Dropout(p=0, inplace=False)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Model\n",
        "---"
      ],
      "metadata": {
        "id": "Jscs1XGUMCKC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "src_vocab_size = 10\n",
        "trg_vocab_size = 10\n",
        "src_pad_idx = 0\n",
        "trg_pad_idx = 0\n",
        "embed_size = 512\n",
        "num_layers = 6\n",
        "heads = 8\n",
        "forward_expansion = 4\n",
        "dropout = 0.1\n",
        "max_length = 100\n",
        "batch_size = 2\n",
        "lr = 3e-4\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "H-FMPbthLLBb"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=trg_pad_idx)\n",
        "optimizer = optim.Adam(model.parameters(), lr=lr)"
      ],
      "metadata": {
        "id": "Jt7zXv3IL-zj"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, optimizer, criterion, device):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for src, trg in train_loader:\n",
        "        src, trg = src.to(device), trg.to(device)\n",
        "\n",
        "        # Zero gradients before backpropagation\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass (only use input seq excluding the last token for the target)\n",
        "        output = model(src, trg[:, :-1])\n",
        "\n",
        "        # Compute loss: output shape = (N, trg_len-1, trg_vocab_size)\n",
        "        # trg[:, 1:].shape = (N, trg_len-1) because we want to compare outputs with targets\n",
        "        output = output.reshape(-1, trg_vocab_size)\n",
        "        trg = trg[:, 1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        # Backpropagate the error and update the model weights\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients to avoid explosion\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Optimizer step\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Training loss: {avg_loss:.4f}\")\n",
        "    return avg_loss\n"
      ],
      "metadata": {
        "id": "LZGDHdp4LS2l"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class DummyDataset(Dataset):\n",
        "    def __init__(self, src_vocab_size, trg_vocab_size, max_len, num_samples=100):\n",
        "        self.src_vocab_size = src_vocab_size\n",
        "        self.trg_vocab_size = trg_vocab_size\n",
        "        self.max_len = max_len\n",
        "        self.num_samples = num_samples\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        src = torch.randint(1, self.src_vocab_size, (self.max_len,))\n",
        "        trg = torch.randint(1, self.trg_vocab_size, (self.max_len,))\n",
        "        return src, trg\n",
        "\n",
        "# Create the dataset and dataloaders\n",
        "train_dataset = DummyDataset(src_vocab_size, trg_vocab_size, max_len=max_length, num_samples=100)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "axXT3oj2LYb0"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize model\n",
        "model = Transformer(src_vocab_size, trg_vocab_size, device=device,\n",
        "                    src_pad_idx=src_pad_idx, trg_pad_idx=trg_pad_idx,\n",
        "                    embed_size=embed_size, num_layers=num_layers,\n",
        "                    forward_expansion=forward_expansion, heads=heads,\n",
        "                    dropout=dropout, max_length=max_length).to(device)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss = train(model, train_loader, optimizer, criterion, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yhF-oUOJLaVC",
        "outputId": "6e41ac74-45ef-4d5a-b762-54d79893a525"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Training loss: 2.4666\n",
            "Epoch 2/10\n",
            "Training loss: 2.4727\n",
            "Epoch 3/10\n",
            "Training loss: 2.4792\n",
            "Epoch 4/10\n",
            "Training loss: 2.4763\n",
            "Epoch 5/10\n",
            "Training loss: 2.4690\n",
            "Epoch 6/10\n",
            "Training loss: 2.4721\n",
            "Epoch 7/10\n",
            "Training loss: 2.4757\n",
            "Epoch 8/10\n",
            "Training loss: 2.4832\n",
            "Epoch 9/10\n",
            "Training loss: 2.4834\n",
            "Epoch 10/10\n",
            "Training loss: 2.4777\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Predict\n",
        "---"
      ],
      "metadata": {
        "id": "qnlonucrNhhP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, src, idx_to_word, max_len=20):\n",
        "    sos_token = 1\n",
        "    trg = torch.ones((src.shape[0], 1)).fill_(sos_token).long().to(device)  # Start with <sos>\n",
        "\n",
        "    # Convert source indices to words\n",
        "    src_text = \" \".join([idx_to_word.get(token.item(), \"<unk>\") for token in src[0]])\n",
        "    print(\"Question: \", \" \".join([word for word in src_text.split() if word != \"<pad>\" and word != \"<unk>\"]))\n",
        "\n",
        "\n",
        "    for t in range(1, max_len):\n",
        "        # Generate the target mask and source mask\n",
        "        trg_mask = model.make_trg_mask(trg)  # Target mask to prevent future information leak\n",
        "        src_mask = model.make_src_mask(src)  # Source mask to avoid padding tokens in source\n",
        "\n",
        "        # Pass the source and target sequence\n",
        "        with torch.no_grad():\n",
        "            output = model(src, trg)  # (batch_size, seq_len, vocab_size)\n",
        "\n",
        "        # Get the last generated token's\n",
        "        next_token = output[:, -1, :].argmax(dim=1).unsqueeze(1)  # Get the most probable token\n",
        "\n",
        "        # Append the predicted token to the target sequence\n",
        "        trg = torch.cat((trg, next_token), dim=1)\n",
        "\n",
        "        eos_token = 8  # Assuming 8 is the <eos> token in your vocabulary\n",
        "        if torch.any(next_token == eos_token):  # Check for <eos> in any token in the batch\n",
        "            break\n",
        "\n",
        "    # Convert predicted target sequence to words and remove <pad>, <unk> and <eos> tokens\n",
        "    predicted_words = [idx_to_word.get(token.item(), \"<unk>\") for token in trg[0]]\n",
        "    answer_text = \" \".join([word for word in predicted_words if word != \"<pad>\" and word != \"<unk>\" and word != \"<eos>\"])\n",
        "\n",
        "    print(\"Answer: \", answer_text)\n",
        "\n",
        "    return trg"
      ],
      "metadata": {
        "id": "G59ADPJGNiuD"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example\n",
        "src_example = torch.tensor([[1, 5, 6, 7, 2], [1, 3, 4, 0, 0]]).to(device)\n",
        "\n",
        "# Convert predicted token indices to words using idx_to_word\n",
        "idx_to_word = {\n",
        "    0: \"<pad>\", 1: \"the\", 2: \"dog\", 3: \"chased\", 4: \"cat\",\n",
        "    5: \"ran\", 6: \"quickly\", 7: \"away\", 8: \"<eos>\", 9: \"<unk>\"\n",
        "}\n",
        "\n",
        "# Generate predicted target sequence\n",
        "predicted_trg = predict(model, src_example, idx_to_word)\n",
        "\n",
        "# Convert predicted token indices to words\n",
        "predicted_words = [idx_to_word.get(token.item(), \"<unk>\") for token in predicted_trg[0]]\n",
        "\n",
        "print(\"Predicted Words:\", \" \".join(predicted_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JK-Jyg0_NkcK",
        "outputId": "59a1f50e-8ca6-45c3-ff6e-8af7cbdb3f2d"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question:  the ran quickly away dog\n",
            "Answer:  the quickly\n",
            "Predicted Words: the quickly <pad> <eos>\n"
          ]
        }
      ]
    }
  ]
}